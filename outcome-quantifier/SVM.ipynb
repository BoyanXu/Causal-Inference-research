{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import unicodedata\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "## sklearn\n",
    "import sklearn.svm as svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "## Scipy\n",
    "from scipy.sparse import csr_matrix"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "data_folder = '../Data/reddit/title/'\n",
    "positive_file_names = ['anxiety', 'ask_reddit', 'depression', 'psychosis', 'stress', 'SuicideWatch']\n",
    "negative_file_names = ['ask_reddit']\n",
    "file_extension = '.txt'\n",
    "\n",
    "# Load positive dataframe\n",
    "pos_df = pd.read_csv(filepath_or_buffer=data_folder + positive_file_names[0] + file_extension, sep='❖', header =None, names =['text'])\n",
    "pos_df['source'] = positive_file_names[0]\n",
    "pos_df['label'] = 1\n",
    "\n",
    "neg_df = pd.read_csv(filepath_or_buffer=data_folder + negative_file_names[0] + file_extension, sep='❖', header =None, names =['text'])\n",
    "## Balance the positive and negative samples\n",
    "neg_df = neg_df.sample(n=pos_df.shape[0], random_state=1, ignore_index=True)\n",
    "neg_df['source'] = negative_file_names[0]\n",
    "neg_df['label'] = -1\n",
    "\n",
    "df = pd.concat([pos_df, neg_df], ignore_index=True)\n",
    "df"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/boyanxu/anaconda3/envs/causal_inference/lib/python3.9/site-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the separator encoded in utf-8 is > 1 char long, and the 'c' engine does not support such separators; you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm worried and angry about a lot of things rn...</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I've never been afraid of anything</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm afraid I am getting agoraphobia</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I think I'm a Hypochondriac</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I start school tomorrow and I haven't done any...</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33995</th>\n",
       "      <td>What's the craziest slow-burning prank with mu...</td>\n",
       "      <td>ask_reddit</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33996</th>\n",
       "      <td>What's a weird thing others say that you do / ...</td>\n",
       "      <td>ask_reddit</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33997</th>\n",
       "      <td>How does VPN work? Can the authorities track y...</td>\n",
       "      <td>ask_reddit</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33998</th>\n",
       "      <td>[Serious] We've known about the risk of global...</td>\n",
       "      <td>ask_reddit</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33999</th>\n",
       "      <td>What's the strangest experience you've had on ...</td>\n",
       "      <td>ask_reddit</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text      source  label\n",
       "0      I'm worried and angry about a lot of things rn...     anxiety      1\n",
       "1                     I've never been afraid of anything     anxiety      1\n",
       "2                    I'm afraid I am getting agoraphobia     anxiety      1\n",
       "3                            I think I'm a Hypochondriac     anxiety      1\n",
       "4      I start school tomorrow and I haven't done any...     anxiety      1\n",
       "...                                                  ...         ...    ...\n",
       "33995  What's the craziest slow-burning prank with mu...  ask_reddit     -1\n",
       "33996  What's a weird thing others say that you do / ...  ask_reddit     -1\n",
       "33997  How does VPN work? Can the authorities track y...  ask_reddit     -1\n",
       "33998  [Serious] We've known about the risk of global...  ask_reddit     -1\n",
       "33999  What's the strangest experience you've had on ...  ask_reddit     -1\n",
       "\n",
       "[34000 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Text preprocessing\n",
    "- Tokenizes\n",
    "- Make text lowercase\n",
    "- Removes hyperlinks\n",
    "- Remove punctuation\n",
    "- Removes numbers\n",
    "- Removes useless words \"stopwords\"\n",
    "- Stemming/Lemmatization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stemmer    = nltk.SnowballStemmer(\"english\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "        Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "        and remove words containing numbers.\n",
    "    '''\n",
    "    text = (unicodedata.normalize('NFKD', text)\n",
    "    .encode('ascii', 'ignore')\n",
    "    .decode('utf-8', 'ignore')\n",
    "    .lower())\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text) # remove urls\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) # remove punctuation\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "def preprocess_data(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    stemmer    = nltk.SnowballStemmer(\"english\")\n",
    "    text = clean_text(text)                                                     # Clean puntuation, urls, and so on\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)    # Remove stopwords\n",
    "    text = ' '.join(stemmer.stem(word) for word in text.split())                # Stemm all the words in the sentence\n",
    "    return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "df['clean_text'] = df.text.apply(preprocess_data)\n",
    "df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm worried and angry about a lot of things rn...</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>1</td>\n",
       "      <td>im worri angri lot thing rn dont know even sor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I've never been afraid of anything</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>1</td>\n",
       "      <td>ive never afraid anyth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm afraid I am getting agoraphobia</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>1</td>\n",
       "      <td>im afraid get agoraphobia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I think I'm a Hypochondriac</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>1</td>\n",
       "      <td>think im hypochondriac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I start school tomorrow and I haven't done any...</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>1</td>\n",
       "      <td>start school tomorrow havent done holiday work...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33995</th>\n",
       "      <td>What's the craziest slow-burning prank with mu...</td>\n",
       "      <td>ask_reddit</td>\n",
       "      <td>-1</td>\n",
       "      <td>what craziest slowburn prank multipl twist bui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33996</th>\n",
       "      <td>What's a weird thing others say that you do / ...</td>\n",
       "      <td>ask_reddit</td>\n",
       "      <td>-1</td>\n",
       "      <td>what weird thing other say done asleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33997</th>\n",
       "      <td>How does VPN work? Can the authorities track y...</td>\n",
       "      <td>ask_reddit</td>\n",
       "      <td>-1</td>\n",
       "      <td>vpn work author track even your use vpn peopl ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33998</th>\n",
       "      <td>[Serious] We've known about the risk of global...</td>\n",
       "      <td>ask_reddit</td>\n",
       "      <td>-1</td>\n",
       "      <td>weve known risk global pandem decad obvious ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33999</th>\n",
       "      <td>What's the strangest experience you've had on ...</td>\n",
       "      <td>ask_reddit</td>\n",
       "      <td>-1</td>\n",
       "      <td>what strangest experi youv date app</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text      source  label  \\\n",
       "0      I'm worried and angry about a lot of things rn...     anxiety      1   \n",
       "1                     I've never been afraid of anything     anxiety      1   \n",
       "2                    I'm afraid I am getting agoraphobia     anxiety      1   \n",
       "3                            I think I'm a Hypochondriac     anxiety      1   \n",
       "4      I start school tomorrow and I haven't done any...     anxiety      1   \n",
       "...                                                  ...         ...    ...   \n",
       "33995  What's the craziest slow-burning prank with mu...  ask_reddit     -1   \n",
       "33996  What's a weird thing others say that you do / ...  ask_reddit     -1   \n",
       "33997  How does VPN work? Can the authorities track y...  ask_reddit     -1   \n",
       "33998  [Serious] We've known about the risk of global...  ask_reddit     -1   \n",
       "33999  What's the strangest experience you've had on ...  ask_reddit     -1   \n",
       "\n",
       "                                              clean_text  \n",
       "0      im worri angri lot thing rn dont know even sor...  \n",
       "1                                 ive never afraid anyth  \n",
       "2                              im afraid get agoraphobia  \n",
       "3                                 think im hypochondriac  \n",
       "4      start school tomorrow havent done holiday work...  \n",
       "...                                                  ...  \n",
       "33995  what craziest slowburn prank multipl twist bui...  \n",
       "33996             what weird thing other say done asleep  \n",
       "33997  vpn work author track even your use vpn peopl ...  \n",
       "33998  weve known risk global pandem decad obvious ca...  \n",
       "33999                what strangest experi youv date app  \n",
       "\n",
       "[34000 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Top n-gram features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "df['split'] = np.random.choice([\"train\", \"val\", \"test\"], size=df.shape[0], p=[.7, .15, .15])\n",
    "x_train = df[df[\"split\"] == \"train\"]\n",
    "y_train = x_train[\"label\"]\n",
    "x_val = df[df[\"split\"] == \"val\"]\n",
    "y_val = x_val[\"label\"]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "## Training pipeline\n",
    "tf_idf = Pipeline([\n",
    "     ('tfidf', TfidfVectorizer(ngram_range=(1,3))),\n",
    "     (\"classifier\", svm.LinearSVC(C=1.0, class_weight=\"balanced\"))\n",
    " ])\n",
    "\n",
    "tf_idf.fit(x_train[\"clean_text\"], y_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer(ngram_range=(1, 3))),\n",
       "                ('classifier', LinearSVC(class_weight='balanced'))])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "## Confidence measure\n",
    "f1_score(y_val, tf_idf.predict(x_val[\"clean_text\"]))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9264305177111716"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "coefs = tf_idf.named_steps[\"classifier\"].coef_\n",
    "if type(coefs) == csr_matrix:\n",
    "    coefs.toarray().tolist()[0]\n",
    "else:\n",
    "    coefs.tolist()\n",
    "    \n",
    "feature_names = tf_idf.named_steps[\"tfidf\"].get_feature_names()\n",
    "coefs_and_features = list(zip(coefs[0], feature_names))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "sorted(coefs_and_features, key=lambda x: x[0], reverse=True)[:50]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(10.20860672237495, 'anxieti'),\n",
       " (7.9422261446705305, 'im'),\n",
       " (5.581288096920053, 'anxious'),\n",
       " (4.380216970539506, 'ive'),\n",
       " (4.186657298060825, 'cant'),\n",
       " (3.0547206139498173, 'feel'),\n",
       " (2.8603748824279878, 'dont know'),\n",
       " (2.6750162896951637, 'worri'),\n",
       " (2.649703556594423, 'help'),\n",
       " (2.557416206385601, 'panic'),\n",
       " (2.4991289933701735, 'what happen'),\n",
       " (2.4846470349412817, 'what wrong'),\n",
       " (2.446857727208464, 'trigger'),\n",
       " (2.431324140146172, 'fear'),\n",
       " (2.382970761909734, 'ssris'),\n",
       " (2.2742412227785387, 'what experi'),\n",
       " (2.252713728622761, 'therapi'),\n",
       " (2.206300612603103, 'scare'),\n",
       " (2.1702891924757375, 'ill'),\n",
       " (2.0931698521452824, 'symptom'),\n",
       " (2.0286588946483306, 'feel like'),\n",
       " (1.9956232773895684, 'freak'),\n",
       " (1.9720864531080824, 'dae'),\n",
       " (1.956815776921675, 'medic'),\n",
       " (1.9513894777338177, 'id'),\n",
       " (1.9352102089035823, 'attack'),\n",
       " (1.8918571132722908, 'overthink'),\n",
       " (1.8405474227632943, 'panic attack'),\n",
       " (1.8381951534720304, 'dont'),\n",
       " (1.7615551577137134, 'med'),\n",
       " (1.7563641876641705, 'stress'),\n",
       " (1.734148865808139, 'afraid'),\n",
       " (1.727916977803465, 'struggl'),\n",
       " (1.7176408234971916, 'terrifi'),\n",
       " (1.6875522426677452, 'overwhelm'),\n",
       " (1.679941122617771, 'breath'),\n",
       " (1.6763333009377615, 'nervous'),\n",
       " (1.6709951141398671, 'what point'),\n",
       " (1.646498113359341, 'hard'),\n",
       " (1.6209319383081313, 'psychiatrist'),\n",
       " (1.6124777256581262, 'anyon els'),\n",
       " (1.6018654443231048, 'anyon'),\n",
       " (1.572352725422819, 'how everyon'),\n",
       " (1.5478165669185342, 'health'),\n",
       " (1.5463301776646643, 'constant'),\n",
       " (1.5331775831248757, 'remind'),\n",
       " (1.5007725756739392, 'panick'),\n",
       " (1.499873130193845, 'what past'),\n",
       " (1.4827182911840497, 'brain'),\n",
       " (1.4493535321406983, 'therapist')]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SVM Classifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "## Build features for clean_text\n",
    "features = [x[1] for x in sorted(coefs_and_features, key=lambda x: x[0], reverse=True)[:5000]]\n",
    "for feature in features:\n",
    "    df[feature] = df.clean_text.str.contains(feature).map(int)\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/var/folders/sz/2_1lr07x18db530kqy41t5fc0000gn/T/ipykernel_75008/1754355366.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[feature] = df.clean_text.str.contains(feature).map(int)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>split</th>\n",
       "      <th>anxieti</th>\n",
       "      <th>im</th>\n",
       "      <th>anxious</th>\n",
       "      <th>ive</th>\n",
       "      <th>cant</th>\n",
       "      <th>...</th>\n",
       "      <th>death dont think</th>\n",
       "      <th>dont think life</th>\n",
       "      <th>think life worth</th>\n",
       "      <th>lay bed</th>\n",
       "      <th>bound cultur</th>\n",
       "      <th>bound cultur parent</th>\n",
       "      <th>cultur parent</th>\n",
       "      <th>cultur parent dont</th>\n",
       "      <th>peer</th>\n",
       "      <th>cringey event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>1</td>\n",
       "      <td>im worri angri lot thing rn dont know even sor...</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>1</td>\n",
       "      <td>ive never afraid anyth</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>1</td>\n",
       "      <td>im afraid get agoraphobia</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>1</td>\n",
       "      <td>think im hypochondriac</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>1</td>\n",
       "      <td>start school tomorrow havent done holiday work...</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5004 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   text   source  label                                         clean_text  \\\n",
       "0     0  anxiety      1  im worri angri lot thing rn dont know even sor...   \n",
       "1     0  anxiety      1                             ive never afraid anyth   \n",
       "2     0  anxiety      1                          im afraid get agoraphobia   \n",
       "3     0  anxiety      1                             think im hypochondriac   \n",
       "4     0  anxiety      1  start school tomorrow havent done holiday work...   \n",
       "\n",
       "   split  anxieti  im  anxious  ive  cant  ...  death dont think  \\\n",
       "0  train        0   1        0    0     0  ...                 0   \n",
       "1  train        0   0        0    1     0  ...                 0   \n",
       "2  train        0   1        0    0     0  ...                 0   \n",
       "3  train        0   1        0    0     0  ...                 0   \n",
       "4  train        0   0        0    0     0  ...                 0   \n",
       "\n",
       "   dont think life  think life worth  lay bed  bound cultur  \\\n",
       "0                0                 0        0             0   \n",
       "1                0                 0        0             0   \n",
       "2                0                 0        0             0   \n",
       "3                0                 0        0             0   \n",
       "4                0                 0        0             0   \n",
       "\n",
       "   bound cultur parent  cultur parent  cultur parent dont  peer  cringey event  \n",
       "0                    0              0                   0     0              0  \n",
       "1                    0              0                   0     0              0  \n",
       "2                    0              0                   0     0              0  \n",
       "3                    0              0                   0     0              0  \n",
       "4                    0              0                   0     0              0  \n",
       "\n",
       "[5 rows x 5004 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "## Build train & test set \n",
    "X = df.drop(columns=['text', 'source', 'label', 'clean_text', 'split'])\n",
    "Y = df.label\n",
    "# X_train,X_test,Y_train,Y_test = train_test_split(X, Y, random_state=20)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "## 5-fold cross validation\n",
    "clf = svm.LinearSVC()\n",
    "scores = cross_val_score(clf, X, Y, cv=5, scoring='f1')\n",
    "scores.mean()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8709743239011238"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "## Save the model\n",
    "with open(positive_file_names[0] + '.sav', 'wb') as sav:\n",
    "    pickle.dump(clf, sav)\n",
    "# loaded_model = pickle.load(open(positive_file_names[0] + '.sav', 'rb'))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('causal_inference': conda)"
  },
  "interpreter": {
   "hash": "2bc74cc49154a9ee15c00e6801fd896878f8e345944270608f40bd45df5df7c1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}